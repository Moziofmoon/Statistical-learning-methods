# 第一章 统计学习及监督学习概论

[TOC]

## 1.1 统计学习

### 1.统计学习的特点		

​		统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。

​		赫尔伯特·西蒙（Herbert A. Simon）对学习的定义：“如果一个系统能够通过执行某个过程改进它的性能，这就是学习。”按照这一观点，统计学习就是计算机系统通过运用数据及统计方法提高系统性能的机器学习。

### 2.统计学习的对象

​		统计学习的研究对象是数据（data）。关于数据的基本假设是数据具有一定的统计规律性。

### 3.统计学习的目的

​		统计学习用于对数据的预测和分析。

### 4.统计学习的方法

​		统计学习的方法是基于数据构建概率统计模型从而对数据进行预测与分析。分类：

- 监督模型（supervised learning）
- 无监督模型（unsupervised learning）
- 强化学习（reinforcement learning）

​		统计学习的方法可以概括为：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space）；应用某个评价准则（evaluation criterion）,从假设空间选取一个最优模型，使他对已知的训练数据及未知的测试数据（test data）在给定的评价准则下有最优的预测。其中最优模型的选取由算法实现。这样，统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法。称其为统计学习方法的三要素，简称模型（model）、策略（strategy）和算法（algorithm）

#### 实现学习方法的步骤：

> 1. 得到一个有限的训练数据集合
> 1. 确定包含所有可能的模型的**假设空间**，即学习模型的集合
> 1. 确定模型选择的准则，即学习的**策略**
> 1. 实现求解最优模型的算法，即学习的**算法**
> 1. 通过学习方法选择最优的模型
> 1. 利用学习的最优模型对新数据进行预测或分析

### 5.统计学习的研究

### 6.统计学习的重要性

> 1. 处理海量数据的有效方法
> 1. 计算机智能化的重要手段
> 1. 计算机学科的重要组成

## 1.2 统计学习的分类

### 1.2.1 基本分类

#### 1.监督学习

​		监督学习（supervised learning）是指从标注数据中学习预测模型的机器学习问题。标注数据表示输入输出的对于关系，预测模型对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的映射的统计规律。

##### （1）输入空间、特征空间和输出空间

- 输入空间（input space）
- 输出空间（output space）
- 特征空间（feature space）

​		每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。这时，所有特征向量存在的空间称为特征空间。模型实际上是定义在特征空间上的。

​		输入$ x $的特征向量：

$$
 x = (x^{(1)} + x^{(2)} + \dots + x^{(n)})^T 
$$

​		注意 $ x^{(i)} $ 的第$i$个特征。注意$x^{(i)}$与$x_i$不同，本书通常用$x_i$表示多个输入变量的第$i$个变量，即：
$$
x_i = (x^{(1)}_i + x^{(2)}_i + \dots + x^{(n)}_i)^T
$$

##### （2） 联合概论分布

​		X 和 Y 具有联合概论分布 P（X，Y）就是监督学习关于数据的基本假设

##### （3）假设空间

​		模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。

​		监督学习的模型可以是概率模型或非概论模型，由条件概论分布$P(Y|X)$或决策函数(decision function)$Y = f(x)$

##### （4）问题的转化

 ![image-20200229161728268](img\image-20200229161728268.png)

#### 2. 无监督学习

​		无监督学习（unsupervised learning）是指从无标注数据中学习预测模型的机器学习模型。本质是学习数据中的统计规律或潜在结构。每一个输出是对输入的分析结果，由输入的类别、转换或概论来表示。包含所有可能的模型的集合称为假设空间。无监督模型旨在从假设空间中选出给定评价标准下的最优模型。

![image-20200229164427398](img\image-20200229164427398.png)

#### 3. 强化学习

​		强化学习（reinforcement learning）是指只能系统在与环境的连续互动中学习最优行为策略的机器学习问题。

### 1.2.2 按模型分类

#### 概率模型和非概率模型

在监督学习中，概率模型是生成模型，非概率模型是判别模型。

感知机、支持向量机、k近邻、Adaboost、k均值、潜在语义分析、神经网络均是非概率模型。

概率图模型，均使用加法规则和乘法规则。

#### 线性模型和非线性模型

#### 参数化模型和非参数化模型

- 参数化模型参数维度固定：感知机、朴素贝叶斯、逻辑回归、kmeans

- 非参数化模型参数的维度不固定：决策树、支持向量机、Adaboost、kNN非参数化模型

### 1.2.3 按算法分类

- 在线学习（online learning）：每次接收一个样本，进行预测，之后学习模型，并不断重复。
- 批量学习（batch learning）：批量通过所有数据，学习模型，之后进行预测。

### 1.2.4 按技巧分类

#### 1.贝叶斯学习（Bayesian learning）

​		在概率模型的学习和推理中，利用贝叶斯定理，计算在给定数据条件下模型的条件概率，即后验概率。

##### 2.核函数

​		核方法（kernel method）是使用核函数表示核学习非线性模型的一种机器学习方法。

## 1.3 统计学习方法的三要素

$$
方法 = 模型 + 策略 + 算法
$$

### 1.3.1 模型

​		在监督学习中，模型就是所要学习的条件概率分布或决策函数。

|              | 假设空间$\cal F$                                             | 输入空间$\cal X$ | 输出空间$\cal Y$ | 参数空间      |
| ------------ | ------------------------------------------------------------ | ---------------- | ---------------- | ------------- |
| 决策函数     | $\cal F\it =\{f_{\theta} |Y=f_{\theta}(x), \theta \in \bf R \it ^n\}$ | 变量             | 变量             | $\bf R\it ^n$ |
| 条件概率分布 | $\cal F\it =\{P|P_{\theta}(Y|X),\theta\in \bf R \it ^n\}$    | 随机变量         | 随机变量         | $\bf R\it ^n$ |

### 1.3.2 策略

#### 1. 损失函数和风险函数

损失函数定义为给定输入$X$的**预测值$f(X)$**和**真实值$Y$**之间的**非负实值**函数，记作$L(Y,f(X))$

##### （1） 0 -1 损失函数

$$
L(Y, f(X)) = \begin{cases}
1& Y \not= f(X)\\
0& Y = f(X)
\end{cases}
$$

##### （2）平方损失函数（quadratic loss function）

$$
L(Y, f(x)) = (Y - f(X))^2
$$

##### （3）绝对值损失函数（absolute loss function）

$$
L(Y, f(x)) = |Y - f(x)|
$$

##### （4）对数损失函数、对数似然损失函数（logarithmic\log-likelihood loss function）

$$
L(Y,P(Y|X)) = - logP(Y|X)
$$



风险函数：
$$
R_{exp}(f)=E_p[L(Y, f(X))]=\int_{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\, {\rm d}x{\rm d}y​
$$
因为计算风险函数需要联合分布P(X|Y)是未知的,所以不能直接计算。

所以由**经验风险**(empirical risk)或**经验损失**(empirical loss)来估计期望风险。
$R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))$
模型$f$关于**训练样本集**的平均损失
根据大数定律，当样本容量N趋于无穷大时，经验风险趋于期望风险。但是由于训练样本有限，所以需要一定的矫正。就需要精要风险最小化和结构风险最小化。

#### 2. 经验风险、结构风险最小化

##### 经验风险最小化（empirical risk minimization, ERM）

$$
\min_{f\in1}\frac{1}{N}L(y_i, f(x_i))
$$

​		当样本容量足够大的时候，ERM能够保证很好的学习效果，如极大似然估计（maximum likelihood estimation）。但是当样本容量很小时，ERM会产生过拟合（over-fitting）现象

​		监督学习的问题就变成了经验风险或结构风险函数最优化的问题。此时经验或结构风险函数就是最优化的目标函数。

##### 结构化风险最小化（structural risk minimization, SRM）

​		SRM是为了防止过拟合而提出的策略，等价于正则化（regularization）。即结构风险是在经验风险上添加表示模型复杂度的正则化项（regularizer）或罚项（penalty term）
$$
R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)
$$
​		$J(f)$为模型复杂度, $\lambda \geqslant 0$是系数，用以权衡经验风险和模型复杂度。

### 1.3.3 算法

## 1.4 模型评估与模型训练

### 1.4.1 训练误差（training error）与测试误差（test error）

### 1.4.2 过拟合与模型选择

## 1.5 模型选择方法：正则化与交叉验证

### 1.5.1 正则化

​		模型选择的典型方法是正则化
$$
\min_{f\in\cal F}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)
$$

- $L_1$范数：$\lambda ||w||_1$
- $L_2$范数：$\frac{\lambda}{2}||w||^2$

​		正则化符合奥卡姆剃刀（Occam's razor）原理：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型。在贝叶斯估计的角度看，正则化对应于模型的先验概率。

### 1.5.2 交叉验证

​		另一种常用的模型选择方法是交叉验证（cross validation）

- 简单
- S折（K折，K-Fold）
- 留一法

## 1.6 泛化能力

### 1.6.1 泛化误差

​		泛化能力（generalization ablity）是指由该方法学习到的模型对未知数据的预测能力。

​		模型对未知数据预测的误差即泛化误差(generalization error):
$$
\begin{align}
R_{exp}(\hat f) &= E_P[L(Y, \hat f(X))]\\
  &= \int_{\mathcal X\times\mathcal Y}L(y,\hat f(x))P(x,y)\, {\rm d}x{\rm d}y​
\end{align}
$$

### 1.6.2 泛化误差上界

​		学习方法的泛化能力往往是通过研究泛化误差的**概率上界**进行的, 简称为泛化误差上界(generalization error bound)

## 1.7 生成模型与判别模型

​		**监督学习方法**可分为**生成方法**(generative approach)与**判别方法**(discriminative approach)，即模型为生成模型（generative model）和判别模型（discriminative model）

### 生成方法

generative approach

- 可以还原出**联合概率分布**$P(X,Y)$
- 收敛速度快, 当样本容量增加时, 学到的模型可以更快收敛到真实模型
- 当存在隐变量时仍可以用

### 判别方法

discriminative approach

- 直接学习**条件概率**$P(Y|X)$或者**决策函数**$f(X)$
- 直接面对预测, 往往学习准确率更高
- 可以对数据进行各种程度的抽象,  定义特征并使用特征, 可以简化学习问题

## 1.8 监督学习应用

### 1.8.1 分类问题

评价指标：

- 准确率（accuracy）:正确样本数与总样本数之比
- 二分类：精确率（precision）与召回率(recall)

TP——将正类预测为正类

FN——将正类预测为负类

FP——将负类预测为正类

TN——将负类预测为负类

精确率：
$$
P = \frac{TP}{TP + FP}
$$
召回率：
$$
R = \frac{TP}{TP + FN}
$$
$F_1$ score:
$$
\frac{2}{F_1}= \frac{1}{P} + \frac{1}{R} \\
F_1 = \frac{2PR}{P + R}
$$

### 1.8.2 标注问题

​		标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。

​		评价指标与分类问题一致。

### 1.8.3 回归问题

​		回归用于预测输入变量（自变量）和输出变量直接的关系。回归模型正是表示从输入变量到输出变量之间映射的函数。

​		分类：一元回归和多元回归、线性回归和非线性回归。

​		损失函数：平方损失函数，一般由最小二乘法求解。

## 习题

### 1. 说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。

极大似然：

![image-20200301170224181](img\image-20200301170224181.png)

贝叶斯：

![image-20200301174148198](img\image-20200301174148198.png)

求极值的计算公式：

![image-20200301174705738](img\image-20200301174705738.png)

### 2 通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。

![image-20200301175541517](img\image-20200301175541517.png)