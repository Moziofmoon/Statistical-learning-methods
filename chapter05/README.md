# 第5章 决策树

	[TOC]

​		决策树（decision tree）是一种基本的分类和回归方法。即可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。

## 5.1 决策树模型与学习

### 5.1.1模型

![image-20200312171430537](img\image-20200312171430537.png)

### 5.1.2 if-then规则

### 5.1.3 条件概率分布

![image-20200312171638788](img\image-20200312171638788.png)

### 5.1.4 决策树学习

​		决策树学习旨在构建一个与训练数据拟合很好，并且复杂度小的决策树。因为从可能的决策树中直接选取最优决策树是NP完全问题。现实中采用启发式方法学习次优的决策树。

## 5.2 特征选择

### 5.2.1 特征选择问题

特征选择在于选取对训练数据具有分类能力的特征

### 5.2.2 信息增益

​		**熵**是表示随机变量不确定性的度量

![image-20200312172833979](img\image-20200312172833979.png)

![image-20200312173042363](img\image-20200312173042363.png)

![image-20200312173121355](img\image-20200312173121355.png)

![image-20200312173214224](img\image-20200312173214224.png)

![image-20200312173339571](img\image-20200312173339571.png)

![image-20200312173407589](img\image-20200312173407589.png)

### 5.2.3 信息增益比

![image-20200312173541644](img\image-20200312173541644.png)

## 5.3 决策树的生成

### 5.3.1 ID3算法

![image-20200316153951983](img\image-20200316153951983.png)



### 5.3.2 C4.5 生成算法

![image-20200316153840740](img\image-20200316153840740.png)

## 5.4 决策树的剪枝

​		在决策树中将已生成的树进行简化的过程称为剪枝（pruning）。通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。

![image-20200316164755580](img\image-20200316164755580.png)

![image-20200316164907407](img\image-20200316164907407.png)

![image-20200316164928896](img\image-20200316164928896.png)

![image-20200316165000677](img\image-20200316165000677.png)

## 5.5 CART算法

​		分类与回归树（classification and regression tree, CART）模型

![image-20200316171252696](img\image-20200316171252696.png)

### 5.5.1 CART生成

![image-20200316171437486](img\image-20200316171437486.png)

![image-20200316171814334](img\image-20200316171814334.png)

![image-20200316171833081](img\image-20200316171833081.png)

![image-20200316172031102](img\image-20200316172031102.png)

### 5.5.2 CART 剪枝

![image-20200316172251805](img\image-20200316172251805.png)